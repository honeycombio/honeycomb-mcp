name: Tests & Evaluation

on:
  push:
    branches: [ main ]
  pull_request:
    # Run on all pull requests, regardless of target branch

# Add permissions to allow PR comments
permissions:
  contents: read
  pull-requests: write
  actions: read

jobs:
  test:
    name: Test
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        node-version: [18.x, 20.x]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Use Node.js ${{ matrix.node-version }}
      uses: actions/setup-node@v4
      with:
        node-version: ${{ matrix.node-version }}
    
    # Setup PNPM - must be before setting up Node.js cache
    - name: Setup PNPM
      uses: pnpm/action-setup@v2
    
    # Setup Node.js cache after PNPM is installed
    - name: Setup Node.js with cache
      uses: actions/setup-node@v4
      with:
        node-version: ${{ matrix.node-version }}
        cache: 'pnpm'
    
    - name: Install dependencies
      run: pnpm install
      
    - name: Typecheck (entire codebase)
      run: pnpm typecheck
    
    - name: Run tests
      run: pnpm test
    
    - name: Run test with coverage
      run: pnpm test:coverage
      
    - name: Build
      run: pnpm build

  # New job that runs after all test matrix jobs complete
  evaluate:
    name: Run Evaluations
    # This job will only run if all test jobs succeed
    needs: test
    runs-on: ubuntu-latest
    # Special handling for main branch
    if: success()
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup PNPM
      uses: pnpm/action-setup@v2
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'pnpm'
    
    - name: Install dependencies
      run: pnpm install
    
    - name: Build project for evaluation
      run: pnpm run build
    
    - name: Configure MCP environment
      run: echo "Using environment variable-based configuration"
    
    # Verify the build file exists before running evals
    - name: Verify build file exists
      run: |
        mkdir -p eval/reports
        if [ ! -f "build/index.mjs" ]; then
          echo "ERROR: build/index.mjs does not exist after build step!"
          echo '<!DOCTYPE html>' > eval/reports/build-failed.html
          echo '<html><head><title>Build Failed</title></head>' >> eval/reports/build-failed.html
          echo '<body><h1>Evaluation Failed</h1>' >> eval/reports/build-failed.html
          echo '<p>The MCP build output file does not exist. Check the build step for errors.</p>' >> eval/reports/build-failed.html
          echo '</body></html>' >> eval/reports/build-failed.html
          exit 1
        else
          echo "Build file found, proceeding with evaluation"
        fi

    - name: Run evaluations
      id: run_evals
      run: |
        echo "Running evaluations..."
        if ! pnpm run eval; then
          echo "::error::Evaluation failed during execution"
          echo "EVAL_OUTCOME=failed" >> $GITHUB_ENV
          # Create a failure report but don't exit yet - we want to collect all artifacts
          mkdir -p eval/reports
          echo '<!DOCTYPE html>' > eval/reports/eval-failed.html
          echo '<html><head><title>Evaluation Failed</title></head>' >> eval/reports/eval-failed.html
          echo '<body><h1>Evaluation Failed</h1>' >> eval/reports/eval-failed.html
          echo '<p>The evaluation process encountered an error. Check the logs for details.</p>' >> eval/reports/eval-failed.html
          echo '<h2>Configuration Information</h2>' >> eval/reports/eval-failed.html
          echo '<pre>' >> eval/reports/eval-failed.html
          if [ -n "$HONEYCOMB_API_KEY" ]; then
            echo "Honeycomb API key is set (length: ${#HONEYCOMB_API_KEY})" >> eval/reports/eval-failed.html
          else
            echo "Honeycomb API key is not set!" >> eval/reports/eval-failed.html
            echo "Make sure HONEYCOMB_API_KEY is set in GitHub secrets and passed to the workflow" >> eval/reports/eval-failed.html
          fi
          echo '</pre>' >> eval/reports/eval-failed.html
          echo '</body></html>' >> eval/reports/eval-failed.html
          # Print environment variables (excluding secrets) for debugging
          echo "Environment variables for debugging:"
          env | grep -v -E "HONEYCOMB_API_KEY|OPENAI_API_KEY|ANTHROPIC_API_KEY" | sort
        else
          echo "EVAL_OUTCOME=success" >> $GITHUB_ENV
        fi
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        # Use Honeycomb API key for environment variable-based config
        HONEYCOMB_API_KEY: ${{ secrets.HONEYCOMB_API_KEY }}
        # Use only limited models for CI to save costs
        EVAL_MODELS: '{"openai":"gpt-4o","anthropic":"claude-3-5-haiku-latest"}'
        EVAL_CONCURRENCY: 2
        EVAL_JUDGE_PROVIDER: "anthropic"
        EVAL_JUDGE_MODEL: "claude-3-5-haiku-latest"
        MCP_SERVER_COMMAND: "node build/index.mjs"
    
    - name: Ensure reports directory exists
      run: mkdir -p eval/reports
      
    - name: Create index file if no reports are generated
      run: |
        # Check if any HTML reports exist
        if [ -z "$(find eval/reports -name '*.html' 2>/dev/null)" ]; then
          echo "No reports were generated, creating a placeholder"
          echo '<!DOCTYPE html>' > eval/reports/no-reports.html
          echo '<html><head><title>No Reports</title></head>' >> eval/reports/no-reports.html
          echo '<body><h1>No evaluation reports generated</h1>' >> eval/reports/no-reports.html
          echo '<p>This could be due to missing API keys or configuration.</p>' >> eval/reports/no-reports.html
          echo '</body></html>' >> eval/reports/no-reports.html
        fi
        
    - name: Find latest report
      id: find-report
      run: |
        LATEST_REPORT=$(ls -t eval/reports/*.html 2>/dev/null | head -1 || echo "eval/reports/no-reports.html")
        echo "latest_report=$LATEST_REPORT" >> $GITHUB_OUTPUT
    
    - name: Post report summary
      run: |
        if [ "$EVAL_OUTCOME" == "failed" ]; then
          echo "## ❌ Evaluation Failed" > $GITHUB_STEP_SUMMARY
          echo "The evaluation process encountered errors. See logs for details." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Error report: $(basename ${{ steps.find-report.outputs.latest_report }})" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "The error report is available as a workflow artifact." >> $GITHUB_STEP_SUMMARY
        else
          echo "## ✅ Evaluation Results" > $GITHUB_STEP_SUMMARY
          echo "Ran evaluations with OpenAI and Anthropic models." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Summary" >> $GITHUB_STEP_SUMMARY
          echo "Latest report: $(basename ${{ steps.find-report.outputs.latest_report }})" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "The full report is available as a workflow artifact." >> $GITHUB_STEP_SUMMARY
        fi
        
        # Add PR comment with detailed summary if we're on a PR
        if [ "${{ github.event_name }}" == "pull_request" ]; then
          PR_COMMENT="## Honeycomb MCP Evaluation Results\n\n"
          
          if [ "$EVAL_OUTCOME" == "failed" ]; then
            PR_COMMENT+="❌ Evaluation process failed\n\n"
            PR_COMMENT+="The evaluation process encountered errors. See workflow logs for details."
          else
            # Find the latest summary file
            LATEST_SUMMARY=$(find eval/results -name "summary-*.json" -type f | sort -r | head -1)
            
            # Extract summary information
            if [ -n "$LATEST_SUMMARY" ] && [ -f "$LATEST_SUMMARY" ]; then
              SUCCESS_RATE=$(jq -r '.successRate * 100 | round(1)' "$LATEST_SUMMARY" 2>/dev/null || echo "N/A")
              PASSED=$(jq -r '.passed' "$LATEST_SUMMARY" 2>/dev/null || echo "N/A")
              TOTAL=$(jq -r '.totalTests' "$LATEST_SUMMARY" 2>/dev/null || echo "N/A")
              
              # Create a more informative completion message
              if [ "$SUCCESS_RATE" != "N/A" ] && [ "$SUCCESS_RATE" -ge 75 ]; then
                PR_COMMENT+="✅ Evaluations completed successfully: **${SUCCESS_RATE}%** pass rate (${PASSED}/${TOTAL} tests)\n\n"
              elif [ "$SUCCESS_RATE" != "N/A" ] && [ "$SUCCESS_RATE" -ge 50 ]; then
                PR_COMMENT+="⚠️ Evaluations completed with mixed results: **${SUCCESS_RATE}%** pass rate (${PASSED}/${TOTAL} tests)\n\n"
              elif [ "$SUCCESS_RATE" != "N/A" ]; then
                PR_COMMENT+="❌ Evaluations completed with poor results: **${SUCCESS_RATE}%** pass rate (${PASSED}/${TOTAL} tests)\n\n"
              else
                PR_COMMENT+="✅ Evaluations completed successfully\n\n"
              fi
            else
              PR_COMMENT+="✅ Evaluations completed successfully\n\n"
            fi
            
            if [ -n "$LATEST_SUMMARY" ] && [ -f "$LATEST_SUMMARY" ]; then
              echo "Found summary file: $LATEST_SUMMARY"
              
              # Parse JSON with jq to extract metrics
              SUCCESS_RATE=$(jq -r '.successRate * 100 | round(1)' "$LATEST_SUMMARY" 2>/dev/null || echo "N/A")
              PASSED=$(jq -r '.passed' "$LATEST_SUMMARY" 2>/dev/null || echo "N/A")
              TOTAL=$(jq -r '.totalTests' "$LATEST_SUMMARY" 2>/dev/null || echo "N/A")
              AVG_LATENCY=$(jq -r '.averageLatency | round' "$LATEST_SUMMARY" 2>/dev/null || echo "N/A")
              
              # Get additional metrics if available
              AVG_TOOL_CALLS=$(jq -r 'if has("averageToolCalls") then .averageToolCalls | round(1) else "N/A" end' "$LATEST_SUMMARY" 2>/dev/null || echo "N/A")
              AVG_TOOL_TOKENS=$(jq -r 'if has("averageToolTokens") then .averageToolTokens | round else "N/A" end' "$LATEST_SUMMARY" 2>/dev/null || echo "N/A")
              
              # Get judge details if available
              JUDGE_PROVIDER=$(jq -r '.metadata.judge.provider // "N/A"' "$LATEST_SUMMARY" 2>/dev/null || echo "N/A")
              JUDGE_MODEL=$(jq -r '.metadata.judge.model // "N/A"' "$LATEST_SUMMARY" 2>/dev/null || echo "N/A")
              
              # Add summary metrics table
              PR_COMMENT+="### Evaluation Summary\n\n"
              PR_COMMENT+="| Metric | Value |\n"
              PR_COMMENT+="|--------|-------|\n"
              PR_COMMENT+="| Success Rate | $SUCCESS_RATE% |\n"
              PR_COMMENT+="| Tests Passed | $PASSED / $TOTAL |\n"
              PR_COMMENT+="| Avg Latency | ${AVG_LATENCY}ms |\n"
              if [ "$AVG_TOOL_CALLS" != "N/A" ]; then
                PR_COMMENT+="| Avg Tool Calls | $AVG_TOOL_CALLS |\n"
              fi
              if [ "$AVG_TOOL_TOKENS" != "N/A" ]; then
                PR_COMMENT+="| Avg Tool Tokens | $AVG_TOOL_TOKENS |\n"
              fi
              
              # Add judge information if available
              if [ "$JUDGE_PROVIDER" != "N/A" ] && [ "$JUDGE_MODEL" != "N/A" ]; then
                PR_COMMENT+="\nValidation performed by: $JUDGE_PROVIDER / $JUDGE_MODEL\n"
              fi
              
              PR_COMMENT+="\n"
            else
              echo "No summary.json file found in eval/results"
              
              # Add placeholder metrics table
              PR_COMMENT+="### Evaluation Summary\n\n"
              PR_COMMENT+="| Metric | Value |\n"
              PR_COMMENT+="|--------|-------|\n"
              PR_COMMENT+="| Success Rate | N/A |\n"
              PR_COMMENT+="| Tests Passed | N/A |\n"
              PR_COMMENT+="| Avg Latency | N/A |\n\n"
            fi
            
            # Add models tested section - extract from summary.json results with test counts
            PR_COMMENT+="### Models Tested\n\n"
            
            if [ -n "$LATEST_SUMMARY" ] && [ -f "$LATEST_SUMMARY" ]; then
              # Create a table for the models used with test counts and success rates
              PR_COMMENT+="| Provider | Model | Tests | Success Rate |\n"
              PR_COMMENT+="|----------|-------|-------|-------------|\n"
              
              # Use jq to extract and count models and their results
              MODEL_SUMMARY=$(jq -r '
              .results | 
              group_by(.provider + "-" + .model) | 
              map({
                provider: .[0].provider, 
                model: .[0].model, 
                count: length, 
                passed: map(select(.validation.passed == true)) | length,
                success_rate: (map(select(.validation.passed == true)) | length) / length * 100
              })' "$LATEST_SUMMARY" 2>/dev/null)
              
              # Check if we successfully extracted model data
              if [ -n "$MODEL_SUMMARY" ] && [ "$MODEL_SUMMARY" != "null" ]; then
                # Loop through each model entry and add to table
                echo "$MODEL_SUMMARY" | jq -c '.[]' | while read -r model_entry; do
                  PROVIDER=$(echo "$model_entry" | jq -r '.provider')
                  MODEL=$(echo "$model_entry" | jq -r '.model')
                  COUNT=$(echo "$model_entry" | jq -r '.count')
                  SUCCESS=$(echo "$model_entry" | jq -r '.success_rate | round(1)')
                  
                  # Convert provider to title case
                  PROVIDER_FORMATTED=$(echo "$PROVIDER" | awk '{print toupper(substr($0,1,1)) tolower(substr($0,2))}')
                  
                  PR_COMMENT+="| $PROVIDER_FORMATTED | $MODEL | $COUNT | ${SUCCESS}% |\n"
                done
              else
                # Fallback to old method if jq query didn't work
                OPENAI_MODELS=$(jq -r '.results[] | select(.provider == "openai") | .model' "$LATEST_SUMMARY" 2>/dev/null | sort -u | tr '\n' ', ' | sed 's/,$//' | sed 's/,/, /g' || echo "None")
                ANTHROPIC_MODELS=$(jq -r '.results[] | select(.provider == "anthropic") | .model' "$LATEST_SUMMARY" 2>/dev/null | sort -u | tr '\n' ', ' | sed 's/,$//' | sed 's/,/, /g' || echo "None")
                
                # Add to the table if models were found
                if [ -n "$OPENAI_MODELS" ] && [ "$OPENAI_MODELS" != "None" ]; then
                  PR_COMMENT+="| OpenAI | ${OPENAI_MODELS} | - | - |\n"
                fi
                
                if [ -n "$ANTHROPIC_MODELS" ] && [ "$ANTHROPIC_MODELS" != "None" ]; then
                  PR_COMMENT+="| Anthropic | ${ANTHROPIC_MODELS} | - | - |\n"
                fi
                
                # If no models were found for either provider
                if [ "$OPENAI_MODELS" == "None" ] && [ "$ANTHROPIC_MODELS" == "None" ]; then
                  PR_COMMENT+="Model information could not be extracted from results\n"
                fi
              fi
            else
              PR_COMMENT+="Model information not available\n"
            fi
            
            # Add test failures section if there are any failures
            if [ -n "$LATEST_SUMMARY" ] && [ -f "$LATEST_SUMMARY" ] && [ "$PASSED" != "$TOTAL" ]; then
              FAILURES=$(jq -r '.results | map(select(.validation.passed == false)) | map({id: .id, provider: .provider, model: .model, score: .validation.score})' "$LATEST_SUMMARY" 2>/dev/null)
              
              if [ -n "$FAILURES" ] && [ "$FAILURES" != "[]" ] && [ "$FAILURES" != "null" ]; then
                PR_COMMENT+="### Failed Tests\n\n"
                PR_COMMENT+="| Test ID | Provider | Model | Score |\n"
                PR_COMMENT+="|--------|----------|-------|-------|\n"
                
                echo "$FAILURES" | jq -c '.[]' | while read -r test; do
                  ID=$(echo "$test" | jq -r '.id')
                  PROVIDER=$(echo "$test" | jq -r '.provider')
                  MODEL=$(echo "$test" | jq -r '.model')
                  SCORE=$(echo "$test" | jq -r 'if has("score") and .score != null then (.score * 100 | round) | tostring + "%" else "N/A" end')
                  
                  # Format provider name
                  PROVIDER_FORMATTED=$(echo "$PROVIDER" | awk '{print toupper(substr($0,1,1)) tolower(substr($0,2))}')
                  
                  PR_COMMENT+="| $ID | $PROVIDER_FORMATTED | $MODEL | $SCORE |\n"
                done
                
                PR_COMMENT+="\n"
              fi
            fi
            
            PR_COMMENT+="\n📊 [View full report in workflow artifacts](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})"
          fi
          
          echo -e "$PR_COMMENT" > pr_comment.txt
          
          gh pr comment ${{ github.event.pull_request.number }} --body-file pr_comment.txt
        fi
      env:
        GH_TOKEN: ${{ github.token }}
    
    # Create report index if it doesn't exist
    - name: Generate report index if needed
      run: |
        if [ ! -f "eval/reports/index.html" ]; then
          echo "Generating index.html for reports using the update-index script"
          pnpm run eval:update-index
        fi
    
    # Upload evaluation reports as artifacts
    - name: Upload evaluation reports
      uses: actions/upload-artifact@v4
      with:
        name: evaluation-reports
        path: eval/reports/
        retention-days: 30
        
    # Final step to fail the job if evaluations failed
    - name: Check final evaluation status
      if: env.EVAL_OUTCOME == 'failed'
      run: |
        echo "::error::Evaluation failed - see artifacts for error report"
        exit 1