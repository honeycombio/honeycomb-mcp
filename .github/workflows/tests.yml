name: Tests & Evaluation

on:
  push:
    branches: [ main ]
  pull_request:
    # Run on all pull requests, regardless of target branch

# Add permissions to allow PR comments
permissions:
  contents: read
  pull-requests: write
  actions: read

jobs:
  test:
    name: Test
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        node-version: [18.x, 20.x]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Use Node.js ${{ matrix.node-version }}
      uses: actions/setup-node@v4
      with:
        node-version: ${{ matrix.node-version }}
    
    # Setup PNPM - must be before setting up Node.js cache
    - name: Setup PNPM
      uses: pnpm/action-setup@v2
    
    # Setup Node.js cache after PNPM is installed
    - name: Setup Node.js with cache
      uses: actions/setup-node@v4
      with:
        node-version: ${{ matrix.node-version }}
        cache: 'pnpm'
    
    - name: Install dependencies
      run: pnpm install
      
    - name: Typecheck (entire codebase)
      run: pnpm typecheck
    
    - name: Run tests
      run: pnpm test
    
    - name: Run test with coverage
      run: pnpm test:coverage
      
    - name: Build
      run: pnpm build

  # New job that runs after all test matrix jobs complete
  evaluate:
    name: Run Evaluations
    # This job will only run if all test jobs succeed
    needs: test
    runs-on: ubuntu-latest
    # Special handling for main branch
    if: success()
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup PNPM
      uses: pnpm/action-setup@v2
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'pnpm'
    
    - name: Install dependencies
      run: pnpm install
    
    - name: Build project for evaluation
      run: pnpm run build
    
    - name: Configure MCP environment
      run: echo "Using environment variable-based configuration"
    
    # Verify the build file exists before running evals
    - name: Verify build file exists
      run: |
        mkdir -p eval/reports
        if [ ! -f "build/index.mjs" ]; then
          echo "ERROR: build/index.mjs does not exist after build step!"
          echo '<!DOCTYPE html>' > eval/reports/build-failed.html
          echo '<html><head><title>Build Failed</title></head>' >> eval/reports/build-failed.html
          echo '<body><h1>Evaluation Failed</h1>' >> eval/reports/build-failed.html
          echo '<p>The MCP build output file does not exist. Check the build step for errors.</p>' >> eval/reports/build-failed.html
          echo '</body></html>' >> eval/reports/build-failed.html
          exit 1
        else
          echo "Build file found, proceeding with evaluation"
        fi

    - name: Run evaluations
      id: run_evals
      run: |
        echo "Running evaluations..."
        if ! pnpm run eval; then
          echo "::error::Evaluation failed during execution"
          echo "EVAL_OUTCOME=failed" >> $GITHUB_ENV
          # Create a failure report but don't exit yet - we want to collect all artifacts
          mkdir -p eval/reports
          echo '<!DOCTYPE html>' > eval/reports/eval-failed.html
          echo '<html><head><title>Evaluation Failed</title></head>' >> eval/reports/eval-failed.html
          echo '<body><h1>Evaluation Failed</h1>' >> eval/reports/eval-failed.html
          echo '<p>The evaluation process encountered an error. Check the logs for details.</p>' >> eval/reports/eval-failed.html
          echo '<h2>Configuration Information</h2>' >> eval/reports/eval-failed.html
          echo '<pre>' >> eval/reports/eval-failed.html
          if [ -n "$HONEYCOMB_API_KEY" ]; then
            echo "Honeycomb API key is set (length: ${#HONEYCOMB_API_KEY})" >> eval/reports/eval-failed.html
          else
            echo "Honeycomb API key is not set!" >> eval/reports/eval-failed.html
            echo "Make sure HONEYCOMB_API_KEY is set in GitHub secrets and passed to the workflow" >> eval/reports/eval-failed.html
          fi
          echo '</pre>' >> eval/reports/eval-failed.html
          echo '</body></html>' >> eval/reports/eval-failed.html
          # Print environment variables (excluding secrets) for debugging
          echo "Environment variables for debugging:"
          env | grep -v -E "HONEYCOMB_API_KEY|OPENAI_API_KEY|ANTHROPIC_API_KEY" | sort
        else
          echo "EVAL_OUTCOME=success" >> $GITHUB_ENV
        fi
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        # Use Honeycomb API key for environment variable-based config
        HONEYCOMB_API_KEY: ${{ secrets.HONEYCOMB_API_KEY }}
        # Use only limited models for CI to save costs
        EVAL_MODELS: '{"openai":"gpt-4o","anthropic":"claude-3-5-haiku-latest"}'
        EVAL_CONCURRENCY: 2
        EVAL_JUDGE_PROVIDER: "anthropic"
        EVAL_JUDGE_MODEL: "claude-3-5-haiku-latest"
        MCP_SERVER_COMMAND: "node build/index.mjs"
    
    - name: Ensure reports directory exists
      run: mkdir -p eval/reports
      
    - name: Create index file if no reports are generated
      run: |
        # Check if any HTML reports exist
        if [ -z "$(find eval/reports -name '*.html' 2>/dev/null)" ]; then
          echo "No reports were generated, creating a placeholder"
          echo '<!DOCTYPE html>' > eval/reports/no-reports.html
          echo '<html><head><title>No Reports</title></head>' >> eval/reports/no-reports.html
          echo '<body><h1>No evaluation reports generated</h1>' >> eval/reports/no-reports.html
          echo '<p>This could be due to missing API keys or configuration.</p>' >> eval/reports/no-reports.html
          echo '</body></html>' >> eval/reports/no-reports.html
        fi
        
    - name: Find latest report
      id: find-report
      run: |
        LATEST_REPORT=$(ls -t eval/reports/*.html 2>/dev/null | head -1 || echo "eval/reports/no-reports.html")
        echo "latest_report=$LATEST_REPORT" >> $GITHUB_OUTPUT
    
    - name: Post report summary
      run: |
        if [ "$EVAL_OUTCOME" == "failed" ]; then
          echo "## ❌ Evaluation Failed" > $GITHUB_STEP_SUMMARY
          echo "The evaluation process encountered errors. See logs for details." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Error report: $(basename ${{ steps.find-report.outputs.latest_report }})" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "The error report is available as a workflow artifact." >> $GITHUB_STEP_SUMMARY
        else
          echo "## ✅ Evaluation Results" > $GITHUB_STEP_SUMMARY
          echo "Ran evaluations with OpenAI and Anthropic models." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Summary" >> $GITHUB_STEP_SUMMARY
          echo "Latest report: $(basename ${{ steps.find-report.outputs.latest_report }})" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "The full report is available as a workflow artifact." >> $GITHUB_STEP_SUMMARY
        fi
        
        # Add PR comment with detailed summary if we're on a PR
        if [ "${{ github.event_name }}" == "pull_request" ]; then
          PR_COMMENT="## Honeycomb MCP Evaluation Results\n\n"
          
          if [ "$EVAL_OUTCOME" == "failed" ]; then
            PR_COMMENT+="❌ Evaluation process failed\n\n"
            PR_COMMENT+="The evaluation process encountered errors. See workflow logs for details."
          else
            PR_COMMENT+="✅ Evaluations completed successfully\n\n"
            
            # Extract key metrics from the report using simple grep/sed
            if [ -f "${{ steps.find-report.outputs.latest_report }}" ]; then
              # Get success rate with grep/sed
              SUCCESS_RATE=$(grep -A1 '<div class="label">Success Rate</div>' "${{ steps.find-report.outputs.latest_report }}" | grep value | sed -E 's/.*<div class="value">([0-9.]+)%.*/\1/g' || echo "N/A")
              
              # Get passed and total tests
              PASSED=$(grep -A1 '<div class="label">Passed</div>' "${{ steps.find-report.outputs.latest_report }}" | grep value | sed -E 's/.*<div class="value success">([0-9]+).*/\1/g' || echo "N/A")
              TOTAL=$(grep -A1 '<div class="label">Total Tests</div>' "${{ steps.find-report.outputs.latest_report }}" | grep value | sed -E 's/.*<div class="value">([0-9]+).*/\1/g' || echo "N/A")
              
              # Get average latency
              AVG_LATENCY=$(grep -A1 '<div class="label">Avg Latency</div>' "${{ steps.find-report.outputs.latest_report }}" | grep value | sed -E 's/.*<div class="value">([0-9]+)ms.*/\1/g' || echo "N/A")
              
              # Add summary metrics table
              PR_COMMENT+="### Evaluation Summary\n\n"
              PR_COMMENT+="| Metric | Value |\n"
              PR_COMMENT+="|--------|-------|\n"
              PR_COMMENT+="| Success Rate | $SUCCESS_RATE% |\n"
              PR_COMMENT+="| Tests Passed | $PASSED / $TOTAL |\n"
              PR_COMMENT+="| Avg Latency | ${AVG_LATENCY}ms |\n\n"
            fi
            
            # Add models tested section
            PR_COMMENT+="### Models Tested\n\n"
            
            # Parse models in a more shell-compatible way
            if [ -n "$EVAL_MODELS" ]; then
              # Extract OpenAI models
              OPENAI_MODELS=$(echo "$EVAL_MODELS" | jq -r '.openai | if . == null then "N/A" elif type == "array" then join(", ") else . end' 2>/dev/null || echo "N/A")
              
              # Extract Anthropic models
              ANTHROPIC_MODELS=$(echo "$EVAL_MODELS" | jq -r '.anthropic | if . == null then "N/A" elif type == "array" then join(", ") else . end' 2>/dev/null || echo "N/A")
              
              # Add to the comment
              PR_COMMENT+="- OpenAI: ${OPENAI_MODELS}\n"
              PR_COMMENT+="- Anthropic: ${ANTHROPIC_MODELS}\n\n"
            else
              PR_COMMENT+="- Model information not available\n\n"
            fi
            
            PR_COMMENT+="📊 [View full report in workflow artifacts](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})"
          fi
          
          echo -e "$PR_COMMENT" > pr_comment.txt
          
          gh pr comment ${{ github.event.pull_request.number }} --body-file pr_comment.txt
        fi
      env:
        GH_TOKEN: ${{ github.token }}
    
    # Create report index if it doesn't exist
    - name: Generate report index if needed
      run: |
        if [ ! -f "eval/reports/index.html" ]; then
          echo "Generating index.html for reports using the update-index script"
          pnpm run eval:update-index
        fi
    
    # Upload evaluation reports as artifacts
    - name: Upload evaluation reports
      uses: actions/upload-artifact@v4
      with:
        name: evaluation-reports
        path: eval/reports/
        retention-days: 30
        
    # Final step to fail the job if evaluations failed
    - name: Check final evaluation status
      if: env.EVAL_OUTCOME == 'failed'
      run: |
        echo "::error::Evaluation failed - see artifacts for error report"
        exit 1